---
title: 《深度学习入门：基于Python的理论与实现》学习笔记
description: 这本书写得通俗易懂，算是入门有个大概方向了
#date: 2018-12-15
updated: 2019-05-15
categories:
- 深度学习
tags:
- 深度学习
- Python
---

花书有点看不下去，打印了一本《[深度学习入门：基于Python的理论与实现](http://www.ituring.com.cn/book/1921)》，边看边写点代码。下面总结的内容有的是书上的，有的是其他地方找的资料。
# 第1章 Python入门
## P15
```
>>> X = np.array([[51, 55], [14, 19], [0, 4]])
>>> X = X.flatten()         # 将X转换为一维数组 
>>> print(X)
[51 55 14 19  0  4]
```
```
>>> X[np.array([0, 2, 4])] # 获取索引为0、2、4的元素
array([51, 14,  0])
```
还可以用来获取满足一定条件的元素：
```
>>> X > 15
array([ True,  True, False,  True, False, False])
>>> X[X>15]
array([51, 55, 19])
```
## P18
matplotlib库里面`plt.legend()`可以使用的参数很多，是用来用来显示图例的。
# 第2章 感知机
都学过。
# 第3章 神经网络
## p43
实现支持Numpy数组的阶跃函数，其中`astype()`方法转换Numpy数组的类型：
```
def step_function(x):
    y = x > 0
    return y.astype(np.int)
>>> x = np.array([-1.0, 1.0, 2.0])
>>> step_function(x)
array([0, 1, 1])
```
## P44
`np.arange(-5.0, 5.0, 0.1)`在−5.0到5.0的范围内，以0.1为单位，生成 NumPy数组（`[-5.0, -4.9,…4.9]`）。  
`plt.ylim()`用来指定y轴的范围
## P51
数组的维数可以通过`np.dim`函数获得：

```
>>> A = np.array([1, 2, 3, 4])
>>> np.ndim(A)
1
```
## P53
这里出现了矩阵乘法，顺便总结一下：  
元素乘法：`np.multiply(a,b)`  
矩阵乘法：`np.dot(a,b)` 或 `np.matmul(a,b)` 或 `a.dot(b)` 或 `a@b`  
唯独注意：`*`，在 `np.array` 中重载为元素乘法，在 `np.matrix` 中重载为矩阵乘法！
## P63
激活函数的使用，一般来说回归问题用恒等函数，分类问题用softmax函数。
## P64
softmax函数的分子是输入信号a<sub>k</sub>的指数函数，分母是所有输入信号的指数函数的和。
我们可以把它定义成如下的Python函数：
```
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```
## P67
因为softmax中指数函数的值容易变得非常大导致溢出，因此可以通过减去输入信号的最大值改进，像下面这样实现softmax函数：
```
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c) # 溢出对策
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```
## P78
可以用numpy中的argmax()获取特定在维度最大值元素的索引，例子：
```
>>> x = np.array([  [0.1, 0.8, 0.1], 
                    [0.3, 0.1, 0.6], 
                    [0.2, 0.5, 0.3], 
                    [0.8, 0.1, 0.1]  ])
>>> y = np.argmax(x, axis=1)
>>> print(y)
[1 2 1 0]
```
# 第4章 神经网络的学习
## 一些单词
cross entropy error 交叉熵误差
mini-batch  小批量
numerical differentiation   数值微分
gradient descent method    梯度下降法


## P87
除了均方误差之外，交叉熵误差（cross entropy error）也经常被用作损失函数。下面，我们来用代码实现交叉熵误差。
```
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```
函数内部在计算`np.log`时，加上了一个微小值`delta`。这是因为，当出现`np.log(0)`时，`np.log(0)`会变为负无限大 的`-inf`。这里的`inf`是无穷大的意思，下面总结一下`inf`。

> Python中可以用如下方式表示正负无穷：
> 
> ```
> float("inf"), float("-inf")
> ```
> 
> 利用`inf`做简单加、乘算术运算仍会得到`inf`
> 
> ```
> >>> 1 + float('inf')
> inf
> >>> 2 * float('inf')
> inf
> ```
> 
> 但是利用`inf`乘以`0`会得到`not-a-number(NaN)`：
> 
> ```
> >>> 0 * float("inf")
> nan
> ```
> 
> 除了`inf`外的其他数除以`inf`，会得到`0`
> 
> ```
> >>> 889 / float('inf')
> 0.0
> >>> float('inf')/float('inf')
> nan
> ```
> 
> 不等式： 当涉及`>`和`<`运算时，所有数都比`-inf`大， 所有数都比`+inf`小，等式：`+inf`和`+inf`相等`-inf`和 `-inf`相等
> 

这样一来就会导致后续计算无法进行。作为保护性对策，添加一个 微小值可以防止负无限大的发生。下面，我们使用`cross_entropy_error(y, t)`进行一些简单的计算。

```
>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> cross_entropy_error(np.array(y), np.array(t)) 
0.51082545709933802
>>>
>>> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
>>> cross_entropy_error(np.array(y), np.array(t))
2.3025840929945458
```
## P89
我们从全部数据中选出一部分，作为全部数据的“近似”。神经网络的学习也是从训练数据中选出一批数据（称为mini-batch，小批量），然后对每个mini-batch进行学习。比如，从60000个训练数据中随机选择100笔，再用这100笔数据进行学习。这种学习方式称为mini-batch学习。 
## P90
使用`np.random.choice()`可以从指定的数字中随机选择想要的数字。比如，`np.random.choice(60000,10)`会从0到59999之间随机选择10个数字。如下面的实际代码所示，我们可以得到一个包含被选数据的索引的数组。
```
>>> np.random.choice(60000, 10)
array([ 8013, 14666, 58210, 23832, 52091, 10153, 8107, 19410, 27260, 21411])
```
## P101
`np.zeros_like(x)`会生成一个形状和`x`相同、所有元素都为0的数组。 
## P116
epoch：1个epoch等于使用训练集中的全部样本训练一次，通俗的讲epoch的值就是整个数据集被轮几次。
# 第5章 误差反向传播法
## P140
mask变量保存了由True/ False构成的NumPy数组。

```
>>> x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )
>>> print(x)
[[ 1.  -0.5]
 [-2.   3. ]]
>>> mask = (x <= 0)
>>> print(mask)
[[False  True]
 [ True False]]
```
## P144
仿射层（Affine Layer）
神经网络中的一个全连接层。仿射（Affine）的意思是前面一层中的每一个神经元都连接到当前层中的每一个神经元。在许多方面，这是神经网络的「标准」层。仿射层通常被加在卷积神经网络或循环神经网络做出最终预测前的输出的顶层。仿射层的一般形式为 y = f(Wx + b)，其中 x 是层输入，w 是参数，b 是一个偏差矢量，f 是一个非线性激活函数。
## P156
collections是Python内建的一个集合模块，提供了许多有用的集合类。
使用`dict`时，Key是无序的。在对`dict`做迭代时，我们无法确定Key的顺序。
如果要保持Key的顺序，可以用`OrderedDict`：
```
>>> from collections import OrderedDict
>>> d = dict([('a', 1), ('b', 2), ('c', 3)])
>>> d # dict的Key是无序的
{'a': 1, 'c': 3, 'b': 2}
>>> od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])
>>> od # OrderedDict的Key是有序的
OrderedDict([('a', 1), ('b', 2), ('c', 3)])
```
注意，`OrderedDict`的Key会按照插入的顺序排列，不是Key本身排序：
```
>>> od = OrderedDict()
>>> od['z'] = 1
>>> od['y'] = 2
>>> od['x'] = 3
>>> od.keys() # 按照插入的Key的顺序返回
['z', 'y', 'x']
```
# 第6章 与学习相关的技巧
感觉这一章的干货是最多的。  
## 6.1参数的更新
介绍了4种方法。  
- 随机梯度下降法（stochastic gradient descent）， 简称SGD。  
- Momentum  
- AdaGrad  
- Adam

## 6.2权重的初始值
在神经网络的学习中，权重初始值非常重要。很多时候权重初始值的设定关系到神经网络的学习能否成功。  
- 激活函数使用sigmoid时，使用Xavier初始值。  
- 激活函数使用ReLU时，使用He初始值。

## 6.3Batch Normalization
中文翻译成批规范化，就是进行使数据分布的均值为0、方差为1的正规化。接着，Batch Norm层会对正规化后的数据进行缩放和平移的变换。  
几乎所有的情况下都是使用Batch Norm时学习进行得更快。  
## 6.4正则化  
### 6.4.2权值衰减
很多过拟合原本就是因为权重参数取值过大才发生的。神经网络的学习目的是减小损失函数的值。这时，例如为损失函数加上权重的平方范数（L2范数）。这样一来，就可以抑制权重变大。 
### 6.4.3Dropout
如果网络的模型变得很复杂，只用权值衰减就难以应对了。  
Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。每传递一次数据，就会随机选择要删除的神经元。然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。  
## 6.5超参数的验证
### 6.5.1验证数据
里要注意的是，不能使用测试数据评估超参数的性能。这一点非常重要，但也容易被忽视。  
为什么不能用测试数据评估超参数的性能呢？这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样的话，可能就会得到不能拟合其他数据、泛化能力低的模型。  
训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据。
## P196
`np.random.shuffle(x)`对给定的数组打乱顺序重新排列。
## P198
`np.random.uniform(low,high,size)`从一个均匀分布`[low,high)`中随机采样，注意定义域是左闭右开，即包含`low`，不包含`high`。  
`size`：输出样本数目，为`int`或元组(`tuple`)类型，例如，`size=(m,n,k)`, 则输出`m * n * k`个样本，缺省时输出1个值。
# 第7章 卷积神经网络
## 卷积（Convolution）
- 滤波器
- 填充（padding）
- 步幅（stride）
- 3维数据的通道方向
- 批处理

批处理这里的4维数据好好看看，pytorch里con2d就是这样的。
## 池化（Pooling）
- Max池化
- Average池化

## im2col
`np.pad()`函数：各个维数上的首部和尾部的填充，具体参数的作用可以看官网的[例子](https://docs.scipy.org/doc/numpy-1.15.4/reference/generated/numpy.pad.html)。  
Numpy中的`transpose`函数，更改多维数组的轴的顺序。  
## 具有代表性的CNN
- LeNet
- AlexNet

# 第8章 深度学习
## 加深层的动机
1. 减少网络的参数数量。一次5 × 5的卷积运算的区域可以由两次3 × 3的卷积运算抵充。并且，
相对于前者的参数数量25（5 × 5），后者一共是18（2 × 3 × 3），通过叠加卷积层，参数数量减少了。
2. 使学习更加高效。与没有加深层的网络相比，
通过加深层，可以减少学习数据，可以将各层要学习的问题分解成容易解决的简单问题，从而高效地进行学习。

## 广为人知的网络
- VGG
- GoogLeNet
- ResNet
